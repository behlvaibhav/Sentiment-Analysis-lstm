{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbH0Vobe4PRs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "import keras\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdaVfrTU433h"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "nuql15A944-e",
    "outputId": "e892b32c-135e-434b-e5f4-bd63197de01c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "gmaD2W795Fwm",
    "outputId": "45c2e65f-d006-46d3-a1af-58821e8509e6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "id": "7c7Moym94PR3",
    "outputId": "c252efe5-fda1-4c64-cbc4-6194ebab731b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_E6oV3lV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H2dI7p3v41m8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcLP8yXG4PSA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cb6n7kmf4PSJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29720</td>\n",
       "      <td>29720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  tweet\n",
       "label              \n",
       "0      29720  29720\n",
       "1       2242   2242"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UClgYXuI4PST"
   },
   "outputs": [],
   "source": [
    "count_class_0, count_class_1 = df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FqH0RnT54PSY"
   },
   "outputs": [],
   "source": [
    "df_class_0 =  df.query('label==0')\n",
    "df_class_1 =  df.query('label==1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CwyCN6l4PSe"
   },
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_under = pd.concat([df_class_0_under, df_class_1],ignore_index=True ,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNc_CDPG4PSo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4484, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BQlslFk4PSz"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2242</td>\n",
       "      <td>2242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  tweet\n",
       "label             \n",
       "0      2242   2242\n",
       "1      2242   2242"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_under.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZhPU_plX4PS9"
   },
   "outputs": [],
   "source": [
    "X = df_under['tweet']\n",
    "Y = df_under['label']\n",
    "Y_org = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79a5bKnV4PTC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" @user man rory didn't show up   #ufcottawa\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eds5ksHJ4PTL"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_features = 10000\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', split=' ', lower=True, char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(X.values)\n",
    "X = tokenizer.texts_to_sequences(X.values)\n",
    "\n",
    "# add padding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X = pad_sequences(X, maxlen=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW_k09Wg4PTS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "giwlGZ1U4PTV",
    "outputId": "7bc33603-61e1-46a6-ff89-45510da26b3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' @user lmfao pathetic #soit   #growup #funny #noonethere #iknowwhoitis ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98±ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f¤\\x97ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82ð\\x9f\\x98\\x82â\\x80¦'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['tweet'], key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgOz7bZo4PTg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vaibhav\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Gj4xPlJ4PTj"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 95)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "QETVpTg04PTm",
    "outputId": "13daf7f5-6b6a-4ccd-efbe-ffa7d1b2c0d6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100)\n",
    "plt.xlabel(\"No. of components\")\n",
    "plt.ylabel(\"cummulative explained Variance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_OZL-_s4PTt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.2, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlTIX2GQ4PTw"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrIIlpF14PT0"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gA4171ow4PT3"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(LSTM(units=40, activation='relu',return_sequences= True, input_shape=(None, 95)))\n",
    "classifier.add(Dropout(rate=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZMP0EYY4PT6"
   },
   "outputs": [],
   "source": [
    "classifier.add(LSTM(units=20, return_sequences= True,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(LSTM(units=20,activation='relu'))\n",
    "classifier.add(Dropout(rate=0.2))\n",
    "classifier.add(Dense(units = 2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DxjIi1fm4PT8"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='rmsprop',metrics=['accuracy'],loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "lgcBrRwk4PT_",
    "outputId": "9066b69b-f175-46d5-fab6-59842141b285"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_e = OneHotEncoder()\n",
    "Y_train_org = Y_train\n",
    "Y_test_org = Y_test\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_test = Y_test.reshape(-1, 1)\n",
    "y_e.fit(Y_train)\n",
    "Y_train = y_e.transform(Y_train)\n",
    "Y_test = y_e.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6834
    },
    "colab_type": "code",
    "id": "sGo4HJi34PUD",
    "outputId": "b93181da-e631-45a3-b950-d85d4d06c989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3587 samples, validate on 897 samples\n",
      "Epoch 1/200\n",
      "3587/3587 [==============================] - 6s 2ms/step - loss: 0.6922 - acc: 0.5659 - val_loss: 0.6891 - val_acc: 0.6945\n",
      "Epoch 2/200\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.6813 - acc: 0.679 - 1s 282us/step - loss: 0.6811 - acc: 0.6774 - val_loss: 0.6619 - val_acc: 0.6996\n",
      "Epoch 3/200\n",
      "3587/3587 [==============================] - 1s 280us/step - loss: 0.6484 - acc: 0.6788 - val_loss: 0.6231 - val_acc: 0.7001\n",
      "Epoch 4/200\n",
      "3587/3587 [==============================] - 1s 314us/step - loss: 0.6384 - acc: 0.6873 - val_loss: 0.6137 - val_acc: 0.6968\n",
      "Epoch 5/200\n",
      "3587/3587 [==============================] - 1s 291us/step - loss: 0.6293 - acc: 0.6924 - val_loss: 0.6102 - val_acc: 0.6906\n",
      "Epoch 6/200\n",
      "3587/3587 [==============================] - 1s 375us/step - loss: 0.6216 - acc: 0.6915 - val_loss: 0.6084 - val_acc: 0.6990\n",
      "Epoch 7/200\n",
      "3587/3587 [==============================] - 1s 313us/step - loss: 0.6193 - acc: 0.6945 - val_loss: 0.6075 - val_acc: 0.6968\n",
      "Epoch 8/200\n",
      "3587/3587 [==============================] - 1s 265us/step - loss: 0.6145 - acc: 0.6974 - val_loss: 0.6075 - val_acc: 0.6979\n",
      "Epoch 9/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.6107 - acc: 0.7004 - val_loss: 0.6121 - val_acc: 0.7035\n",
      "Epoch 10/200\n",
      "3587/3587 [==============================] - 1s 237us/step - loss: 0.6065 - acc: 0.7004 - val_loss: 0.6118 - val_acc: 0.7085\n",
      "Epoch 11/200\n",
      "3587/3587 [==============================] - 1s 238us/step - loss: 0.6012 - acc: 0.7080 - val_loss: 0.6130 - val_acc: 0.7029\n",
      "Epoch 12/200\n",
      "3587/3587 [==============================] - 1s 302us/step - loss: 0.5964 - acc: 0.7078 - val_loss: 0.6129 - val_acc: 0.7074\n",
      "Epoch 13/200\n",
      "3587/3587 [==============================] - 1s 321us/step - loss: 0.6006 - acc: 0.7106 - val_loss: 0.6110 - val_acc: 0.7090\n",
      "Epoch 14/200\n",
      "3587/3587 [==============================] - 1s 292us/step - loss: 0.5935 - acc: 0.7127 - val_loss: 0.6137 - val_acc: 0.7113\n",
      "Epoch 15/200\n",
      "3587/3587 [==============================] - 1s 287us/step - loss: 0.5909 - acc: 0.7117 - val_loss: 0.6167 - val_acc: 0.7057\n",
      "Epoch 16/200\n",
      "3587/3587 [==============================] - 1s 248us/step - loss: 0.5906 - acc: 0.7154 - val_loss: 0.6136 - val_acc: 0.7146\n",
      "Epoch 17/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.5908 - acc: 0.7144 - val_loss: 0.6132 - val_acc: 0.7202\n",
      "Epoch 18/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.5858 - acc: 0.7175 - val_loss: 0.6123 - val_acc: 0.7185\n",
      "Epoch 19/200\n",
      "3587/3587 [==============================] - 1s 242us/step - loss: 0.5829 - acc: 0.7269 - val_loss: 0.6086 - val_acc: 0.7213\n",
      "Epoch 20/200\n",
      "3587/3587 [==============================] - 1s 233us/step - loss: 0.5844 - acc: 0.7194 - val_loss: 0.6088 - val_acc: 0.7207\n",
      "Epoch 21/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.5819 - acc: 0.7279 - val_loss: 0.6075 - val_acc: 0.7213\n",
      "Epoch 22/200\n",
      "3587/3587 [==============================] - 1s 217us/step - loss: 0.5786 - acc: 0.7211 - val_loss: 0.6040 - val_acc: 0.7207\n",
      "Epoch 23/200\n",
      "3587/3587 [==============================] - 1s 220us/step - loss: 0.5760 - acc: 0.7293 - val_loss: 0.6079 - val_acc: 0.7207\n",
      "Epoch 24/200\n",
      "3587/3587 [==============================] - 1s 240us/step - loss: 0.5748 - acc: 0.7283 - val_loss: 0.6039 - val_acc: 0.7202\n",
      "Epoch 25/200\n",
      "3587/3587 [==============================] - 1s 237us/step - loss: 0.5714 - acc: 0.7329 - val_loss: 0.6050 - val_acc: 0.7263\n",
      "Epoch 26/200\n",
      "3587/3587 [==============================] - 1s 236us/step - loss: 0.5673 - acc: 0.7340 - val_loss: 0.6075 - val_acc: 0.7235\n",
      "Epoch 27/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.5634 - acc: 0.7407 - val_loss: 0.6038 - val_acc: 0.7230\n",
      "Epoch 28/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.5709 - acc: 0.7307 - val_loss: 0.6173 - val_acc: 0.7269\n",
      "Epoch 29/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.5639 - acc: 0.7435 - val_loss: 0.6125 - val_acc: 0.7252\n",
      "Epoch 30/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.5597 - acc: 0.7414 - val_loss: 0.6095 - val_acc: 0.7297\n",
      "Epoch 31/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.5604 - acc: 0.7379 - val_loss: 0.6093 - val_acc: 0.7285\n",
      "Epoch 32/200\n",
      "3587/3587 [==============================] - 1s 231us/step - loss: 0.5586 - acc: 0.7405 - val_loss: 0.6144 - val_acc: 0.7224\n",
      "Epoch 33/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.5517 - acc: 0.7492 - val_loss: 0.6068 - val_acc: 0.7324\n",
      "Epoch 34/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.5525 - acc: 0.7451 - val_loss: 0.6102 - val_acc: 0.7235\n",
      "Epoch 35/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.5496 - acc: 0.7470 - val_loss: 0.6147 - val_acc: 0.7280\n",
      "Epoch 36/200\n",
      "3587/3587 [==============================] - 1s 212us/step - loss: 0.5538 - acc: 0.7424 - val_loss: 0.6104 - val_acc: 0.7252\n",
      "Epoch 37/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.5457 - acc: 0.7463 - val_loss: 0.6296 - val_acc: 0.7269\n",
      "Epoch 38/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.5518 - acc: 0.7441 - val_loss: 0.6228 - val_acc: 0.7258\n",
      "Epoch 39/200\n",
      "3587/3587 [==============================] - 1s 233us/step - loss: 0.5424 - acc: 0.7517 - val_loss: 0.6336 - val_acc: 0.7246\n",
      "Epoch 40/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.5432 - acc: 0.7526 - val_loss: 0.6247 - val_acc: 0.7341\n",
      "Epoch 41/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.5402 - acc: 0.7529 - val_loss: 0.6265 - val_acc: 0.7330\n",
      "Epoch 42/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.5401 - acc: 0.7494 - val_loss: 0.6363 - val_acc: 0.7297\n",
      "Epoch 43/200\n",
      "3587/3587 [==============================] - 1s 219us/step - loss: 0.5408 - acc: 0.7524 - val_loss: 0.6280 - val_acc: 0.7341\n",
      "Epoch 44/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.5421 - acc: 0.7522 - val_loss: 0.6180 - val_acc: 0.7363\n",
      "Epoch 45/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.5390 - acc: 0.7537 - val_loss: 0.6285 - val_acc: 0.7391\n",
      "Epoch 46/200\n",
      "3587/3587 [==============================] - 1s 212us/step - loss: 0.5299 - acc: 0.7569 - val_loss: 0.6534 - val_acc: 0.7274\n",
      "Epoch 47/200\n",
      "3587/3587 [==============================] - 1s 208us/step - loss: 0.5328 - acc: 0.7607 - val_loss: 0.6456 - val_acc: 0.7302\n",
      "Epoch 48/200\n",
      "3587/3587 [==============================] - 1s 207us/step - loss: 0.5325 - acc: 0.7572 - val_loss: 0.6399 - val_acc: 0.7363\n",
      "Epoch 49/200\n",
      "3587/3587 [==============================] - 1s 203us/step - loss: 0.5288 - acc: 0.7586 - val_loss: 0.6372 - val_acc: 0.7313\n",
      "Epoch 50/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.5234 - acc: 0.7628 - val_loss: 0.6492 - val_acc: 0.7397\n",
      "Epoch 51/200\n",
      "3587/3587 [==============================] - 1s 205us/step - loss: 0.5236 - acc: 0.7647 - val_loss: 0.6363 - val_acc: 0.7347\n",
      "Epoch 52/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.5185 - acc: 0.7650 - val_loss: 0.6378 - val_acc: 0.7408\n",
      "Epoch 53/200\n",
      "3587/3587 [==============================] - 1s 210us/step - loss: 0.5253 - acc: 0.7672 - val_loss: 0.6379 - val_acc: 0.7380\n",
      "Epoch 54/200\n",
      "3587/3587 [==============================] - 1s 208us/step - loss: 0.5246 - acc: 0.7641 - val_loss: 0.6341 - val_acc: 0.7302\n",
      "Epoch 55/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.5209 - acc: 0.7635 - val_loss: 0.6337 - val_acc: 0.7414\n",
      "Epoch 56/200\n",
      "3587/3587 [==============================] - 1s 220us/step - loss: 0.5359 - acc: 0.7534 - val_loss: 0.6214 - val_acc: 0.7402\n",
      "Epoch 57/200\n",
      "3587/3587 [==============================] - 1s 241us/step - loss: 0.5207 - acc: 0.7650 - val_loss: 0.6356 - val_acc: 0.7302\n",
      "Epoch 58/200\n",
      "3587/3587 [==============================] - 1s 240us/step - loss: 0.5234 - acc: 0.7660 - val_loss: 0.6339 - val_acc: 0.7347\n",
      "Epoch 59/200\n",
      "3587/3587 [==============================] - 1s 275us/step - loss: 0.5146 - acc: 0.7714 - val_loss: 0.6376 - val_acc: 0.7402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200\n",
      "3587/3587 [==============================] - 1s 253us/step - loss: 0.5179 - acc: 0.7579 - val_loss: 0.6468 - val_acc: 0.7352\n",
      "Epoch 61/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.5149 - acc: 0.7641 - val_loss: 0.6329 - val_acc: 0.7419\n",
      "Epoch 62/200\n",
      "3587/3587 [==============================] - 1s 247us/step - loss: 0.5151 - acc: 0.7732 - val_loss: 0.6320 - val_acc: 0.7369\n",
      "Epoch 63/200\n",
      "3587/3587 [==============================] - 1s 226us/step - loss: 0.5113 - acc: 0.7717 - val_loss: 0.6414 - val_acc: 0.7319\n",
      "Epoch 64/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.5163 - acc: 0.7644 - val_loss: 0.6342 - val_acc: 0.7425\n",
      "Epoch 65/200\n",
      "3587/3587 [==============================] - 1s 211us/step - loss: 0.5098 - acc: 0.7662 - val_loss: 0.6275 - val_acc: 0.7363\n",
      "Epoch 66/200\n",
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.5045 - acc: 0.7689 - val_loss: 0.6259 - val_acc: 0.7419\n",
      "Epoch 67/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.5063 - acc: 0.7687 - val_loss: 0.6360 - val_acc: 0.7441\n",
      "Epoch 68/200\n",
      "3587/3587 [==============================] - 1s 228us/step - loss: 0.4930 - acc: 0.7781 - val_loss: 0.6337 - val_acc: 0.7414\n",
      "Epoch 69/200\n",
      "3587/3587 [==============================] - 1s 236us/step - loss: 0.4980 - acc: 0.7789 - val_loss: 0.6450 - val_acc: 0.7397\n",
      "Epoch 70/200\n",
      "3587/3587 [==============================] - 1s 230us/step - loss: 0.4956 - acc: 0.7800 - val_loss: 0.6587 - val_acc: 0.7358\n",
      "Epoch 71/200\n",
      "3587/3587 [==============================] - 1s 233us/step - loss: 0.5030 - acc: 0.7793 - val_loss: 0.6457 - val_acc: 0.7436\n",
      "Epoch 72/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.4914 - acc: 0.7793 - val_loss: 0.6442 - val_acc: 0.7436\n",
      "Epoch 73/200\n",
      "3587/3587 [==============================] - 1s 211us/step - loss: 0.4939 - acc: 0.7821 - val_loss: 0.6449 - val_acc: 0.7414\n",
      "Epoch 74/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.4865 - acc: 0.7816 - val_loss: 0.6445 - val_acc: 0.7358\n",
      "Epoch 75/200\n",
      "3587/3587 [==============================] - 1s 211us/step - loss: 0.4857 - acc: 0.7796 - val_loss: 0.6410 - val_acc: 0.7425\n",
      "Epoch 76/200\n",
      "3587/3587 [==============================] - 1s 228us/step - loss: 0.4822 - acc: 0.7869 - val_loss: 0.6409 - val_acc: 0.7464\n",
      "Epoch 77/200\n",
      "3587/3587 [==============================] - 1s 300us/step - loss: 0.4842 - acc: 0.7820 - val_loss: 0.6455 - val_acc: 0.7391\n",
      "Epoch 78/200\n",
      "3587/3587 [==============================] - 1s 211us/step - loss: 0.4750 - acc: 0.7904 - val_loss: 0.6375 - val_acc: 0.7386\n",
      "Epoch 79/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.4759 - acc: 0.7883 - val_loss: 0.6570 - val_acc: 0.7336\n",
      "Epoch 80/200\n",
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.4679 - acc: 0.7943 - val_loss: 0.6657 - val_acc: 0.7369\n",
      "Epoch 81/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.4764 - acc: 0.7890 - val_loss: 0.6650 - val_acc: 0.7414\n",
      "Epoch 82/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4707 - acc: 0.7888 - val_loss: 0.6737 - val_acc: 0.7347\n",
      "Epoch 83/200\n",
      "3587/3587 [==============================] - 1s 226us/step - loss: 0.4739 - acc: 0.7814 - val_loss: 0.6688 - val_acc: 0.7408\n",
      "Epoch 84/200\n",
      "3587/3587 [==============================] - 1s 225us/step - loss: 0.4750 - acc: 0.7881 - val_loss: 0.6707 - val_acc: 0.7391\n",
      "Epoch 85/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4596 - acc: 0.7945 - val_loss: 0.6902 - val_acc: 0.7375\n",
      "Epoch 86/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.4595 - acc: 0.7954 - val_loss: 0.7038 - val_acc: 0.7380\n",
      "Epoch 87/200\n",
      "3587/3587 [==============================] - 1s 257us/step - loss: 0.4626 - acc: 0.7931 - val_loss: 0.6807 - val_acc: 0.7352\n",
      "Epoch 88/200\n",
      "3587/3587 [==============================] - 1s 278us/step - loss: 0.4608 - acc: 0.7952 - val_loss: 0.6889 - val_acc: 0.7330\n",
      "Epoch 89/200\n",
      "3587/3587 [==============================] - 1s 247us/step - loss: 0.4577 - acc: 0.8012 - val_loss: 0.6822 - val_acc: 0.7358\n",
      "Epoch 90/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.4566 - acc: 0.8028 - val_loss: 0.6972 - val_acc: 0.7375\n",
      "Epoch 91/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.4555 - acc: 0.7959 - val_loss: 0.7094 - val_acc: 0.7330\n",
      "Epoch 92/200\n",
      "3587/3587 [==============================] - 1s 244us/step - loss: 0.4587 - acc: 0.7957 - val_loss: 0.7131 - val_acc: 0.7363\n",
      "Epoch 93/200\n",
      "3587/3587 [==============================] - 1s 230us/step - loss: 0.4544 - acc: 0.7966 - val_loss: 0.7395 - val_acc: 0.7375\n",
      "Epoch 94/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4516 - acc: 0.7994 - val_loss: 0.7374 - val_acc: 0.7352\n",
      "Epoch 95/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4682 - acc: 0.7941 - val_loss: 0.7219 - val_acc: 0.7280\n",
      "Epoch 96/200\n",
      "3587/3587 [==============================] - 1s 207us/step - loss: 0.4531 - acc: 0.7965 - val_loss: 0.7331 - val_acc: 0.7324\n",
      "Epoch 97/200\n",
      "3587/3587 [==============================] - 1s 206us/step - loss: 0.4537 - acc: 0.7937 - val_loss: 0.7001 - val_acc: 0.7391\n",
      "Epoch 98/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4441 - acc: 0.8057 - val_loss: 0.7093 - val_acc: 0.7397\n",
      "Epoch 99/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4432 - acc: 0.8061 - val_loss: 0.7193 - val_acc: 0.7402\n",
      "Epoch 100/200\n",
      "3587/3587 [==============================] - 1s 237us/step - loss: 0.4520 - acc: 0.7979 - val_loss: 0.7213 - val_acc: 0.7386\n",
      "Epoch 101/200\n",
      "3587/3587 [==============================] - 1s 219us/step - loss: 0.4549 - acc: 0.7994 - val_loss: 0.7353 - val_acc: 0.7414\n",
      "Epoch 102/200\n",
      "3587/3587 [==============================] - 1s 210us/step - loss: 0.4454 - acc: 0.8025 - val_loss: 0.7612 - val_acc: 0.7363\n",
      "Epoch 103/200\n",
      "3587/3587 [==============================] - 1s 205us/step - loss: 0.4485 - acc: 0.8007 - val_loss: 0.7612 - val_acc: 0.7347\n",
      "Epoch 104/200\n",
      "3587/3587 [==============================] - 1s 210us/step - loss: 0.4402 - acc: 0.8026 - val_loss: 0.7809 - val_acc: 0.7330\n",
      "Epoch 105/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4384 - acc: 0.8061 - val_loss: 0.7873 - val_acc: 0.7397\n",
      "Epoch 106/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4615 - acc: 0.7940 - val_loss: 0.7369 - val_acc: 0.7330\n",
      "Epoch 107/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4381 - acc: 0.8100 - val_loss: 0.7769 - val_acc: 0.7363\n",
      "Epoch 108/200\n",
      "3587/3587 [==============================] - 1s 212us/step - loss: 0.4402 - acc: 0.8078 - val_loss: 0.7725 - val_acc: 0.7347\n",
      "Epoch 109/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4467 - acc: 0.8039 - val_loss: 0.7687 - val_acc: 0.7380\n",
      "Epoch 110/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4515 - acc: 0.8011 - val_loss: 0.7623 - val_acc: 0.7391\n",
      "Epoch 111/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4432 - acc: 0.7991 - val_loss: 0.7452 - val_acc: 0.7380\n",
      "Epoch 112/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.4446 - acc: 0.8007 - val_loss: 0.7765 - val_acc: 0.7408\n",
      "Epoch 113/200\n",
      "3587/3587 [==============================] - 1s 207us/step - loss: 0.4373 - acc: 0.8061 - val_loss: 0.7655 - val_acc: 0.7402\n",
      "Epoch 114/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.4391 - acc: 0.8090 - val_loss: 0.7712 - val_acc: 0.7397\n",
      "Epoch 115/200\n",
      "3587/3587 [==============================] - 1s 207us/step - loss: 0.4326 - acc: 0.8078 - val_loss: 0.7895 - val_acc: 0.7347\n",
      "Epoch 116/200\n",
      "3587/3587 [==============================] - 1s 208us/step - loss: 0.4372 - acc: 0.8114 - val_loss: 0.7975 - val_acc: 0.7336\n",
      "Epoch 117/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.4444 - acc: 0.8069 - val_loss: 0.7822 - val_acc: 0.7324\n",
      "Epoch 118/200\n",
      "3587/3587 [==============================] - 1s 217us/step - loss: 0.4229 - acc: 0.8111 - val_loss: 0.7998 - val_acc: 0.7347\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 240us/step - loss: 0.4419 - acc: 0.8054 - val_loss: 0.7606 - val_acc: 0.7336\n",
      "Epoch 120/200\n",
      "3587/3587 [==============================] - 1s 225us/step - loss: 0.4410 - acc: 0.8089 - val_loss: 0.7547 - val_acc: 0.7414\n",
      "Epoch 121/200\n",
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.4374 - acc: 0.8099 - val_loss: 0.7296 - val_acc: 0.7436\n",
      "Epoch 122/200\n",
      "3587/3587 [==============================] - 1s 226us/step - loss: 0.4299 - acc: 0.8132 - val_loss: 0.7550 - val_acc: 0.7453\n",
      "Epoch 123/200\n",
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.4295 - acc: 0.8053 - val_loss: 0.7599 - val_acc: 0.7475\n",
      "Epoch 124/200\n",
      "3587/3587 [==============================] - 1s 228us/step - loss: 0.4350 - acc: 0.8078 - val_loss: 0.7293 - val_acc: 0.7447\n",
      "Epoch 125/200\n",
      "3587/3587 [==============================] - 1s 221us/step - loss: 0.4382 - acc: 0.8057 - val_loss: 0.7742 - val_acc: 0.7453\n",
      "Epoch 126/200\n",
      "3587/3587 [==============================] - 1s 288us/step - loss: 0.4322 - acc: 0.8082 - val_loss: 0.7643 - val_acc: 0.7447\n",
      "Epoch 127/200\n",
      "3587/3587 [==============================] - 1s 300us/step - loss: 0.4249 - acc: 0.8113 - val_loss: 0.7824 - val_acc: 0.7425\n",
      "Epoch 128/200\n",
      "3587/3587 [==============================] - 1s 230us/step - loss: 0.4266 - acc: 0.8058 - val_loss: 0.7894 - val_acc: 0.7391\n",
      "Epoch 129/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.4234 - acc: 0.8154 - val_loss: 0.7732 - val_acc: 0.7458\n",
      "Epoch 130/200\n",
      "3587/3587 [==============================] - 1s 226us/step - loss: 0.4241 - acc: 0.8092 - val_loss: 0.8063 - val_acc: 0.7447\n",
      "Epoch 131/200\n",
      "3587/3587 [==============================] - 1s 262us/step - loss: 0.4230 - acc: 0.8117 - val_loss: 0.7872 - val_acc: 0.7402\n",
      "Epoch 132/200\n",
      "3587/3587 [==============================] - 1s 270us/step - loss: 0.4326 - acc: 0.8093 - val_loss: 0.7718 - val_acc: 0.7425\n",
      "Epoch 133/200\n",
      "3587/3587 [==============================] - 1s 262us/step - loss: 0.4397 - acc: 0.8111 - val_loss: 0.7881 - val_acc: 0.7436\n",
      "Epoch 134/200\n",
      "3587/3587 [==============================] - 1s 266us/step - loss: 0.4305 - acc: 0.8132 - val_loss: 0.7693 - val_acc: 0.7469\n",
      "Epoch 135/200\n",
      "3587/3587 [==============================] - 1s 264us/step - loss: 0.4275 - acc: 0.8124 - val_loss: 0.8142 - val_acc: 0.7436\n",
      "Epoch 136/200\n",
      "3587/3587 [==============================] - 1s 266us/step - loss: 0.4327 - acc: 0.8122 - val_loss: 0.7837 - val_acc: 0.7414\n",
      "Epoch 137/200\n",
      "3587/3587 [==============================] - 1s 262us/step - loss: 0.4311 - acc: 0.8111 - val_loss: 0.7693 - val_acc: 0.7414\n",
      "Epoch 138/200\n",
      "3587/3587 [==============================] - 1s 234us/step - loss: 0.4117 - acc: 0.8210 - val_loss: 0.8090 - val_acc: 0.7408\n",
      "Epoch 139/200\n",
      "3587/3587 [==============================] - 1s 219us/step - loss: 0.4250 - acc: 0.8078 - val_loss: 0.7791 - val_acc: 0.7425\n",
      "Epoch 140/200\n",
      "3587/3587 [==============================] - 1s 220us/step - loss: 0.4253 - acc: 0.8136 - val_loss: 0.7520 - val_acc: 0.7453\n",
      "Epoch 141/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.4198 - acc: 0.8166 - val_loss: 0.7717 - val_acc: 0.7425\n",
      "Epoch 142/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4185 - acc: 0.8154 - val_loss: 0.7980 - val_acc: 0.7458\n",
      "Epoch 143/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4187 - acc: 0.8156 - val_loss: 0.8123 - val_acc: 0.7414\n",
      "Epoch 144/200\n",
      "3587/3587 [==============================] - 1s 232us/step - loss: 0.4379 - acc: 0.8058 - val_loss: 0.7572 - val_acc: 0.7436\n",
      "Epoch 145/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4153 - acc: 0.8138 - val_loss: 0.7920 - val_acc: 0.7436\n",
      "Epoch 146/200\n",
      "3587/3587 [==============================] - 1s 234us/step - loss: 0.4222 - acc: 0.8149 - val_loss: 0.7798 - val_acc: 0.7464\n",
      "Epoch 147/200\n",
      "3587/3587 [==============================] - 1s 217us/step - loss: 0.4176 - acc: 0.8166 - val_loss: 0.7910 - val_acc: 0.7503\n",
      "Epoch 148/200\n",
      "3587/3587 [==============================] - 1s 217us/step - loss: 0.4257 - acc: 0.8115 - val_loss: 0.8260 - val_acc: 0.7503\n",
      "Epoch 149/200\n",
      "3587/3587 [==============================] - 1s 241us/step - loss: 0.4178 - acc: 0.8113 - val_loss: 0.7848 - val_acc: 0.7458\n",
      "Epoch 150/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.4268 - acc: 0.8097 - val_loss: 0.7684 - val_acc: 0.7480\n",
      "Epoch 151/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4084 - acc: 0.8195 - val_loss: 0.8170 - val_acc: 0.7475\n",
      "Epoch 152/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4212 - acc: 0.8054 - val_loss: 0.7869 - val_acc: 0.7469\n",
      "Epoch 153/200\n",
      "3587/3587 [==============================] - 1s 215us/step - loss: 0.4101 - acc: 0.8195 - val_loss: 0.8251 - val_acc: 0.7447\n",
      "Epoch 154/200\n",
      "3587/3587 [==============================] - 1s 219us/step - loss: 0.4089 - acc: 0.8207 - val_loss: 0.7902 - val_acc: 0.7386\n",
      "Epoch 155/200\n",
      "3587/3587 [==============================] - 1s 225us/step - loss: 0.4176 - acc: 0.8152 - val_loss: 0.7951 - val_acc: 0.7464\n",
      "Epoch 156/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.4149 - acc: 0.8150 - val_loss: 0.8577 - val_acc: 0.7425\n",
      "Epoch 157/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4178 - acc: 0.8113 - val_loss: 0.7977 - val_acc: 0.7520\n",
      "Epoch 158/200\n",
      "3587/3587 [==============================] - 1s 212us/step - loss: 0.4090 - acc: 0.8152 - val_loss: 0.8085 - val_acc: 0.7447\n",
      "Epoch 159/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4168 - acc: 0.8198 - val_loss: 0.8225 - val_acc: 0.7414\n",
      "Epoch 160/200\n",
      "3587/3587 [==============================] - 1s 206us/step - loss: 0.4183 - acc: 0.8043 - val_loss: 0.8022 - val_acc: 0.7547\n",
      "Epoch 161/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.3993 - acc: 0.8235 - val_loss: 0.8501 - val_acc: 0.7480\n",
      "Epoch 162/200\n",
      "3587/3587 [==============================] - 1s 205us/step - loss: 0.4160 - acc: 0.8141 - val_loss: 0.8432 - val_acc: 0.7570\n",
      "Epoch 163/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4187 - acc: 0.8181 - val_loss: 0.7945 - val_acc: 0.7469\n",
      "Epoch 164/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4042 - acc: 0.8196 - val_loss: 0.8555 - val_acc: 0.7458\n",
      "Epoch 165/200\n",
      "3587/3587 [==============================] - 1s 205us/step - loss: 0.4070 - acc: 0.8159 - val_loss: 0.8744 - val_acc: 0.7436\n",
      "Epoch 166/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4008 - acc: 0.8244 - val_loss: 0.8830 - val_acc: 0.7436\n",
      "Epoch 167/200\n",
      "3587/3587 [==============================] - 1s 209us/step - loss: 0.4113 - acc: 0.8193 - val_loss: 0.8587 - val_acc: 0.7480\n",
      "Epoch 168/200\n",
      "3587/3587 [==============================] - 1s 203us/step - loss: 0.4049 - acc: 0.8228 - val_loss: 0.8085 - val_acc: 0.7536\n",
      "Epoch 169/200\n",
      "3587/3587 [==============================] - 1s 219us/step - loss: 0.4083 - acc: 0.8143 - val_loss: 0.8264 - val_acc: 0.7514\n",
      "Epoch 170/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.4003 - acc: 0.8242 - val_loss: 0.8163 - val_acc: 0.7503\n",
      "Epoch 171/200\n",
      "3587/3587 [==============================] - 1s 221us/step - loss: 0.4021 - acc: 0.8177 - val_loss: 0.8160 - val_acc: 0.7480\n",
      "Epoch 172/200\n",
      "3587/3587 [==============================] - 1s 213us/step - loss: 0.4086 - acc: 0.8184 - val_loss: 0.8130 - val_acc: 0.7492\n",
      "Epoch 173/200\n",
      "3587/3587 [==============================] - 1s 202us/step - loss: 0.4013 - acc: 0.8259 - val_loss: 0.8433 - val_acc: 0.7536\n",
      "Epoch 174/200\n",
      "3587/3587 [==============================] - 1s 205us/step - loss: 0.4106 - acc: 0.8235 - val_loss: 0.7937 - val_acc: 0.7447\n",
      "Epoch 175/200\n",
      "3587/3587 [==============================] - 1s 214us/step - loss: 0.4090 - acc: 0.8178 - val_loss: 0.8286 - val_acc: 0.7547\n",
      "Epoch 176/200\n",
      "3587/3587 [==============================] - 1s 211us/step - loss: 0.4020 - acc: 0.8185 - val_loss: 0.8477 - val_acc: 0.7536\n",
      "Epoch 177/200\n",
      "3587/3587 [==============================] - 1s 200us/step - loss: 0.4032 - acc: 0.8235 - val_loss: 0.8531 - val_acc: 0.7536\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.4029 - acc: 0.8196 - val_loss: 0.8516 - val_acc: 0.7480\n",
      "Epoch 179/200\n",
      "3587/3587 [==============================] - 1s 244us/step - loss: 0.3957 - acc: 0.8231 - val_loss: 0.8753 - val_acc: 0.7475\n",
      "Epoch 180/200\n",
      "3587/3587 [==============================] - 1s 242us/step - loss: 0.3990 - acc: 0.8248 - val_loss: 0.8913 - val_acc: 0.7536\n",
      "Epoch 181/200\n",
      "3587/3587 [==============================] - 1s 224us/step - loss: 0.4038 - acc: 0.8187 - val_loss: 0.8718 - val_acc: 0.7559\n",
      "Epoch 182/200\n",
      "3587/3587 [==============================] - 1s 230us/step - loss: 0.4047 - acc: 0.8149 - val_loss: 0.8917 - val_acc: 0.7447\n",
      "Epoch 183/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.3947 - acc: 0.8244 - val_loss: 0.9165 - val_acc: 0.7480\n",
      "Epoch 184/200\n",
      "3587/3587 [==============================] - 1s 266us/step - loss: 0.3984 - acc: 0.8214 - val_loss: 0.9003 - val_acc: 0.7525\n",
      "Epoch 185/200\n",
      "3587/3587 [==============================] - 1s 236us/step - loss: 0.3991 - acc: 0.8185 - val_loss: 0.8655 - val_acc: 0.7492\n",
      "Epoch 186/200\n",
      "3587/3587 [==============================] - 1s 231us/step - loss: 0.3925 - acc: 0.8273 - val_loss: 0.9257 - val_acc: 0.7547\n",
      "Epoch 187/200\n",
      "3587/3587 [==============================] - 1s 233us/step - loss: 0.3945 - acc: 0.8255 - val_loss: 0.9422 - val_acc: 0.7458\n",
      "Epoch 188/200\n",
      "3587/3587 [==============================] - 1s 238us/step - loss: 0.4005 - acc: 0.8214 - val_loss: 0.9237 - val_acc: 0.7480\n",
      "Epoch 189/200\n",
      "3587/3587 [==============================] - 1s 239us/step - loss: 0.4084 - acc: 0.8156 - val_loss: 0.8925 - val_acc: 0.7480\n",
      "Epoch 190/200\n",
      "3587/3587 [==============================] - 1s 227us/step - loss: 0.3932 - acc: 0.8248 - val_loss: 0.8719 - val_acc: 0.7503\n",
      "Epoch 191/200\n",
      "3587/3587 [==============================] - 1s 232us/step - loss: 0.4017 - acc: 0.8175 - val_loss: 0.9156 - val_acc: 0.7592 0s - loss: 0.4043 -\n",
      "Epoch 192/200\n",
      "3587/3587 [==============================] - ETA: 0s - loss: 0.4055 - acc: 0.823 - 1s 233us/step - loss: 0.4054 - acc: 0.8233 - val_loss: 0.8820 - val_acc: 0.7536\n",
      "Epoch 193/200\n",
      "3587/3587 [==============================] - 1s 228us/step - loss: 0.3939 - acc: 0.8231 - val_loss: 0.8790 - val_acc: 0.7503\n",
      "Epoch 194/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.3990 - acc: 0.8251 - val_loss: 0.8942 - val_acc: 0.7592\n",
      "Epoch 195/200\n",
      "3587/3587 [==============================] - 1s 229us/step - loss: 0.4035 - acc: 0.8221 - val_loss: 0.8868 - val_acc: 0.7592\n",
      "Epoch 196/200\n",
      "3587/3587 [==============================] - 1s 216us/step - loss: 0.3925 - acc: 0.8319 - val_loss: 0.8820 - val_acc: 0.7581\n",
      "Epoch 197/200\n",
      "3587/3587 [==============================] - 1s 223us/step - loss: 0.4009 - acc: 0.8238 - val_loss: 0.8720 - val_acc: 0.7598\n",
      "Epoch 198/200\n",
      "3587/3587 [==============================] - 1s 222us/step - loss: 0.4022 - acc: 0.8207 - val_loss: 0.8859 - val_acc: 0.7603\n",
      "Epoch 199/200\n",
      "3587/3587 [==============================] - 1s 218us/step - loss: 0.4018 - acc: 0.8233 - val_loss: 0.8886 - val_acc: 0.7614\n",
      "Epoch 200/200\n",
      "3587/3587 [==============================] - 1s 225us/step - loss: 0.3889 - acc: 0.8259 - val_loss: 0.8728 - val_acc: 0.7642\n"
     ]
    }
   ],
   "source": [
    "X_train_lstm = np.reshape(X_train, (X_train.shape[0],1, X_train.shape[1]))\n",
    "X_test_lstm = np.reshape(X_test, (X_test.shape[0],1 ,X_test.shape[1]))\n",
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    checker = classifier.fit(X_train_lstm, Y_train, batch_size=32, epochs=200, validation_data = (X_test_lstm, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuRIH0Mu4PUH"
   },
   "outputs": [],
   "source": [
    "Y_pred_test_label = classifier.predict(X_test_lstm)\n",
    "y_pred_test=np.argmax(Y_pred_test_label,axis =1)\n",
    "y_pred_test\n",
    "Y_pred_train_label = classifier.predict(X_train_lstm)\n",
    "y_pred_train = np.argmax(Y_pred_train_label,axis=1)\n",
    "Y_test_true = Y_test_org.astype(np.int)\n",
    "Y_train_true = Y_train_org.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "7h6-ufM24PUM",
    "outputId": "f6776684-2683-45c3-88e8-9bf3c8c6ae68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.74      0.76       443\n",
      "          1       0.76      0.78      0.77       454\n",
      "\n",
      "avg / total       0.76      0.76      0.76       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test_true,y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mqtFfYq4PUP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "2Hw1b3Jf4PUU",
    "outputId": "978923d2-7734-4360-dc87-64a5fb4c6640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN:  \n",
      " [[1400  222]\n",
      " [ 399 1566]]\n",
      "\n",
      "Test:  \n",
      " [[329  98]\n",
      " [114 356]]\n"
     ]
    }
   ],
   "source": [
    "### for softmax function\n",
    "print(\"TRAIN:  \\n\",confusion_matrix(y_pred_train,Y_train_true))\n",
    "print(\"\\nTest:  \\n\",confusion_matrix(y_pred_test,Y_test_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "6sbPDlpW4PUZ",
    "outputId": "233b2a2a-e422-4c64-d1cb-f6f9ab8bfec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.78      0.82      1799\n",
      "          1       0.80      0.88      0.83      1788\n",
      "\n",
      "avg / total       0.83      0.83      0.83      3587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_train_true,y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ClHiT1K4PUj"
   },
   "outputs": [],
   "source": [
    "classifier.save(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfxebo8U4PUo"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model(\"rnn_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "j5LQTgpn4PUv",
    "outputId": "bcc51caa-9265-4786-eaac-3db93c8984c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, None, 40)          21760     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 40)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          4880      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 20)                3280      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 29,962\n",
      "Trainable params: 29,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2839
    },
    "colab_type": "code",
    "id": "2WhxPkOH4PU4",
    "outputId": "7c7388a9-95db-4957-f170-e9c25fb56101"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.15638681, -0.05721815,  0.19148135, ...,  0.04406988,\n",
       "         -0.16360393, -0.35887724],\n",
       "        [ 0.3166293 ,  0.17514984, -0.13871841, ...,  0.21283135,\n",
       "          0.40430394,  0.51955664],\n",
       "        [ 0.32493255, -0.1303084 ,  0.07280492, ...,  0.08605286,\n",
       "         -0.09871028, -0.2569402 ],\n",
       "        ...,\n",
       "        [ 0.002373  , -0.00099783,  0.06451716, ...,  0.13219467,\n",
       "          0.04280247,  0.05158597],\n",
       "        [ 0.01398076,  0.01039676,  0.11269689, ..., -0.06204052,\n",
       "          0.06789814,  0.11017343],\n",
       "        [-0.12271531,  0.03535141,  0.10404226, ...,  0.05649738,\n",
       "         -0.05230553,  0.05346039]], dtype=float32),\n",
       " array([[ 0.07994584, -0.050658  , -0.06358485, ...,  0.03923859,\n",
       "          0.04858167,  0.01616315],\n",
       "        [ 0.00535981, -0.01240391,  0.0148801 , ...,  0.07670946,\n",
       "          0.12110909, -0.10349266],\n",
       "        [ 0.09330834,  0.06845205,  0.04006262, ...,  0.03643649,\n",
       "          0.19453183, -0.14908938],\n",
       "        ...,\n",
       "        [-0.25845215, -0.01702788,  0.15413286, ..., -0.12657984,\n",
       "          0.00505872, -0.0050513 ],\n",
       "        [-0.07456384,  0.06924462, -0.0467411 , ..., -0.08635531,\n",
       "         -0.04935603, -0.04794171],\n",
       "        [-0.0659919 ,  0.02258076,  0.01750142, ..., -0.06813749,\n",
       "         -0.02154068, -0.09752197]], dtype=float32),\n",
       " array([-0.2639955 , -0.08266265, -0.04966054,  0.18419062, -0.09351077,\n",
       "         0.40811026,  0.13470986,  0.14254767, -0.02354013,  0.26545644,\n",
       "         0.03175092, -0.05056258, -0.15028238,  0.01579109, -0.36121812,\n",
       "        -0.2588472 ,  0.03255644,  0.0311219 ,  0.30295402, -0.1082547 ,\n",
       "         0.31438047,  0.10891473, -0.2201844 , -0.17394713, -0.25431696,\n",
       "        -0.16141637, -0.11912069, -0.00434557,  0.06252667,  0.00707389,\n",
       "         0.09824504, -0.2225208 , -0.31415585, -0.47672796, -0.12407438,\n",
       "         0.01800396,  0.4839122 ,  0.2536525 ,  0.0708813 ,  0.27142185,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -0.07807411,  0.05042342, -0.49944782, -0.2818491 , -0.21894217,\n",
       "         0.27858707, -0.10980545, -0.3678149 , -0.23886992, -0.36831456,\n",
       "        -0.36860391, -0.01543031, -0.22353122, -0.43159705,  0.2662588 ,\n",
       "        -0.07199235, -0.22514975, -0.45095924, -0.1933561 , -0.26181728,\n",
       "        -0.24535997, -0.42635676,  0.10867231,  0.03401428,  0.08237781,\n",
       "        -0.35622904, -0.07989544, -0.32764837, -0.55895925, -0.28628293,\n",
       "        -0.5022126 , -0.3433585 , -0.20496249,  0.14858331, -0.48360065,\n",
       "        -1.0198543 , -0.22652823, -0.53091776, -0.32727978, -0.50104696,\n",
       "        -0.27338105, -0.31339166, -0.12917233,  0.32703146, -0.04745988,\n",
       "        -0.04233491, -0.05320925,  0.15416695, -0.04846213,  0.43857467,\n",
       "         0.12326403, -0.13545762, -0.07025208,  0.08047686, -0.27399683,\n",
       "        -0.37517288, -0.22126517,  0.01807608,  0.06899318,  0.05692758,\n",
       "         0.08478406,  0.26365468, -0.33522752, -0.03550624, -0.2759302 ,\n",
       "        -0.05563106, -0.00891057,  0.15907848,  0.18750836, -0.12041941,\n",
       "         0.26829663,  0.0563536 , -0.13474776, -0.15031247,  0.02550204,\n",
       "        -0.02231787,  0.08717228, -0.01030856,  0.08075827,  0.2716685 ],\n",
       "       dtype=float32),\n",
       " array([[ 1.1703339 ,  0.15422598, -0.39748335, ...,  0.3361134 ,\n",
       "          0.8987969 , -0.17770752],\n",
       "        [ 0.7934584 , -0.24386288, -0.9552035 , ...,  0.40140057,\n",
       "          0.6418955 , -0.3955983 ],\n",
       "        [ 1.2949841 , -0.30961353, -0.08804289, ...,  0.63855344,\n",
       "          0.6305533 , -0.04192567],\n",
       "        ...,\n",
       "        [ 0.5404087 , -0.00945762,  0.0899892 , ..., -0.10081086,\n",
       "          0.28464028, -0.38308758],\n",
       "        [ 0.65921366, -0.13232571, -0.40680024, ...,  0.3523423 ,\n",
       "          0.47634026, -0.2364308 ],\n",
       "        [ 1.0026041 ,  0.257555  ,  0.5376878 , ...,  0.42213503,\n",
       "         -0.38376957,  0.06821325]], dtype=float32),\n",
       " array([[ 0.11335107, -0.0774029 , -0.01559217, ..., -0.03858694,\n",
       "         -0.24345823,  0.11834969],\n",
       "        [ 0.07682606, -0.04448988, -0.13017678, ..., -0.05202012,\n",
       "         -0.04215335,  0.06265544],\n",
       "        [ 0.12463301,  0.02878283, -0.08353451, ..., -0.15351962,\n",
       "          0.00294397, -0.02773841],\n",
       "        ...,\n",
       "        [-0.09738468, -0.04624192, -0.0182154 , ...,  0.11879879,\n",
       "         -0.00230504, -0.05436448],\n",
       "        [-0.23494491, -0.13348576,  0.08976841, ..., -0.03849688,\n",
       "          0.02637214, -0.10841806],\n",
       "        [ 0.04117684,  0.0608578 , -0.0006614 , ...,  0.04028926,\n",
       "         -0.04725953, -0.02922933]], dtype=float32),\n",
       " array([ 0.43966246,  0.76281166,  1.0799835 ,  0.49888477,  0.87024385,\n",
       "         1.0692168 ,  1.714417  , -0.9466933 ,  1.4105123 ,  0.63503253,\n",
       "         1.1369509 ,  0.0712532 ,  1.1726731 , -0.43577948,  0.9501415 ,\n",
       "         0.92532396,  0.998101  ,  1.0317757 ,  0.9220831 ,  0.79961705,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -0.1328716 ,  0.2921097 ,  0.09068895, -0.08491317, -0.01960133,\n",
       "         0.20486298, -0.23201904, -0.27130824,  0.34350058, -0.40671343,\n",
       "         0.1307572 , -0.24364196, -0.06708986, -0.27925912, -0.0856635 ,\n",
       "         0.29038137,  0.33882236, -0.00798002,  0.3596641 ,  0.28240147,\n",
       "         0.26558226,  0.8313056 ,  1.0117542 ,  0.07861251,  0.15114374,\n",
       "         0.99878895,  0.46024957, -1.1752486 ,  1.4137453 , -0.06030695,\n",
       "         1.1396154 ,  0.00411225,  0.27443534, -0.72772294,  0.0938103 ,\n",
       "         0.9470201 ,  0.9291311 ,  1.1160649 ,  0.8356055 ,  0.607294  ],\n",
       "       dtype=float32),\n",
       " array([[ 1.5113926 ,  1.4565784 ,  0.9007642 , ...,  1.4791788 ,\n",
       "          0.52981925,  1.4148835 ],\n",
       "        [ 1.1827122 ,  0.08685078,  0.9127917 , ...,  0.00619913,\n",
       "         -0.18369241,  0.4039507 ],\n",
       "        [ 0.6894307 , -0.44297305,  0.34504253, ..., -0.2531399 ,\n",
       "         -0.4797676 , -0.3015403 ],\n",
       "        ...,\n",
       "        [ 0.34011889, -0.0604885 ,  0.4086815 , ...,  0.02576777,\n",
       "         -0.24385546, -0.131531  ],\n",
       "        [ 0.24323773, -0.4787834 ,  0.48968637, ..., -0.29079276,\n",
       "         -1.0981936 , -0.7137733 ],\n",
       "        [ 0.8312995 , -0.11676861,  0.4755832 , ...,  0.10417922,\n",
       "         -0.1184079 , -0.01494054]], dtype=float32),\n",
       " array([[-0.06961013,  0.02775044, -0.06558377, ..., -0.03152694,\n",
       "         -0.05120591,  0.0060828 ],\n",
       "        [ 0.09663898, -0.0457672 , -0.09428733, ..., -0.02818732,\n",
       "         -0.06054511, -0.03333944],\n",
       "        [ 0.1026543 , -0.070499  ,  0.05068606, ..., -0.3270012 ,\n",
       "          0.02616684, -0.08692858],\n",
       "        ...,\n",
       "        [ 0.04125995,  0.19178814, -0.07012272, ...,  0.03403358,\n",
       "          0.01905484, -0.02213832],\n",
       "        [-0.13397317,  0.00902924,  0.0917791 , ..., -0.05154881,\n",
       "          0.15860474, -0.0045161 ],\n",
       "        [ 0.13971618,  0.11710145, -0.06046466, ...,  0.05137136,\n",
       "         -0.06838843, -0.21971929]], dtype=float32),\n",
       " array([ 0.75633395,  0.6744798 ,  0.65019935,  0.70430195,  0.29210353,\n",
       "         0.02143264,  0.4183019 ,  0.28281295,  0.8977547 ,  0.6615166 ,\n",
       "        -0.15784557,  0.5510966 ,  0.687724  ,  0.9775032 ,  1.1021098 ,\n",
       "         1.1661736 ,  0.78569525,  0.7367132 , -0.00346755,  0.38331735,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         0.21681887,  0.19429699,  0.2653065 ,  0.22945954,  0.03788641,\n",
       "         0.11123671,  0.26067668,  0.25662577,  0.29428786,  0.24810605,\n",
       "         0.18132778,  0.24017444,  0.24031053,  0.24622318,  0.2174455 ,\n",
       "         0.26833066,  0.23389864,  0.22844172,  0.11728055,  0.17848341,\n",
       "         0.6285591 ,  0.65581876,  0.89879966,  1.4147681 ,  0.36896977,\n",
       "         0.53577924,  0.6189721 ,  0.2936007 ,  0.74075574,  0.7117314 ,\n",
       "         0.7697981 ,  0.46344864,  0.6898718 ,  0.75199074,  1.3231215 ,\n",
       "         1.353214  ,  0.4848843 ,  0.86976033,  0.46527073,  0.5486087 ],\n",
       "       dtype=float32),\n",
       " array([[-0.83256054,  0.8286356 ],\n",
       "        [ 0.82816684, -0.7992978 ],\n",
       "        [-0.5127517 ,  0.44168082],\n",
       "        [ 0.5778617 , -0.6483035 ],\n",
       "        [ 0.82058203, -0.75086516],\n",
       "        [ 0.6700704 , -0.6274344 ],\n",
       "        [-0.38448635,  0.43559852],\n",
       "        [ 0.25505137, -0.26673037],\n",
       "        [-0.61917746,  0.69119185],\n",
       "        [-0.74056965,  0.7038213 ],\n",
       "        [ 0.9323713 , -1.0435928 ],\n",
       "        [ 0.4519524 , -0.5040438 ],\n",
       "        [-0.67949903,  0.674692  ],\n",
       "        [-0.65177476,  0.6457071 ],\n",
       "        [ 0.9390461 , -0.78673047],\n",
       "        [ 1.2021152 , -1.255437  ],\n",
       "        [-0.6215574 ,  0.61185235],\n",
       "        [ 0.95440954, -0.9509112 ],\n",
       "        [ 0.8513555 , -0.80294853],\n",
       "        [ 0.3772547 , -0.361433  ]], dtype=float32),\n",
       " array([-0.36128548,  0.36059085], dtype=float32)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mxOhZ0144PU_",
    "outputId": "af94fa25-14ab-469c-a305-55df36f8f93d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.RMSprop at 0x254e7bdaa90>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "4QZcepbW4PVE",
    "outputId": "0a388400-1c98-4341-82a6-7366c1feeda7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9995816e-01, 5.9688715e-05],\n",
       "       [2.1278006e-01, 7.8744978e-01],\n",
       "       [9.8046827e-01, 1.9982109e-02],\n",
       "       ...,\n",
       "       [1.6654980e-01, 8.3357579e-01],\n",
       "       [1.7659269e-01, 8.2338834e-01],\n",
       "       [3.5599971e-01, 6.4425606e-01]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(X_train_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANL4Cen94PVJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9EAHI344PVO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sentiment_rnn_sampling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
